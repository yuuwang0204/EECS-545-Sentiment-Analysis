{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e480ac7-165b-4182-ae1f-f181f692669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8baf2f0-5703-404e-bd07-8113fda50aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_excel('data_3500.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd7c07d-b226-458b-a70b-fa6086517709",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "\n",
    "with open('stopwords.txt','r',encoding = 'utf8') as f:\n",
    "    for w in f:\n",
    "        stopwords.append(w.strip())\n",
    "        \n",
    "def load_corpus(path):\n",
    "    \"\"\"\n",
    "    加载语料库\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            [_, seniment, content] = line.split(\",\", 2)\n",
    "            content = processing(content)\n",
    "            data.append((content, int(seniment)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_corpus_bert(path):\n",
    "    \"\"\"\n",
    "    加载语料库\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            [_, seniment, content] = line.split(\",\", 2)\n",
    "            content = processing_bert(content)\n",
    "            data.append((content, int(seniment)))\n",
    "    return data\n",
    "\n",
    "def get_stopword_list(file):\n",
    "    with open(file,'r',encoding = 'utf-8') as f:\n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "        return stopword_list\n",
    "    \n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = ''\n",
    "    word_list = jieba.lcut(str)\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result += w\n",
    "    return result\n",
    "    \n",
    "def processing(text):\n",
    "    \"\"\"\n",
    "    数据预处理, 可以根据自己的需求进行重载\n",
    "    \"\"\"\n",
    "    # 数据清洗部分\n",
    "    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # 去除 {%xxx%} (地理定位, 微博话题等)\n",
    "    text = re.sub(\"@.+?( |$)\", \" \", text)           # 去除 @xxx (用户名)\n",
    "    text = re.sub(\"【.+?】\", \" \", text)              # 去除 【xx】 (里面的内容通常都不是用户自己写的)\n",
    "    text = re.sub(\"\\u200b\", \" \", text)              # '\\u200b'是这个数据集中的一个bad case, 不用特别在意\n",
    "    # 分词\n",
    "    words = [w for w in jieba.lcut(text) if w.isalpha()]\n",
    "    # 对否定词`不`做特殊处理: 与其后面的词进行拼接\n",
    "    while \"不\" in words:\n",
    "        index = words.index(\"不\")\n",
    "        if index == len(words) - 1:\n",
    "            break\n",
    "        words[index: index+2] = [\"\".join(words[index: index+2])]  # 列表切片赋值的酷炫写法\n",
    "    # 用空格拼接成字符串\n",
    "    result = \" \".join(words)\n",
    "    return result\n",
    "\n",
    "\n",
    "def processing_bert(text):\n",
    "    \"\"\"\n",
    "    数据预处理, 可以根据自己的需求进行重载\n",
    "    \"\"\"\n",
    "    # 数据清洗部分\n",
    "    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # 去除 {%xxx%} (地理定位, 微博话题等)\n",
    "    text = re.sub(\"@.+?( |$)\", \" \", text)           # 去除 @xxx (用户名)\n",
    "    text = re.sub(\"【.+?】\", \" \", text)              # 去除 【xx】 (里面的内容通常都不是用户自己写的)            # '\\u200b'是这个数据集中的一个bad case, 不用特别在意\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1b10f3-c29a-4928-aa12-fb51499a3db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuwan\\AppData\\Local\\Temp/ipykernel_9844/4125483682.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['context'][i] = processing(df_train['context'][i])\n",
      "C:\\Users\\yuwan\\AppData\\Local\\Temp/ipykernel_9844/4125483682.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['context'][i] = clean_stopword(df_train['context'][i],stopwords)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(df_train)):\n",
    "    df_train['context'][i] = processing(df_train['context'][i])\n",
    "    df_train['context'][i] = clean_stopword(df_train['context'][i],stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36acf9bd-e244-4df3-b306-e69fbfa62b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>time</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>肖战 期待 冬奥 赛场    抹 中国 红 加油 加油</td>\n",
       "      <td>2022-02-04 08:58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北京 冬奥会 闭幕式 期待 下次 冬奥</td>\n",
       "      <td>2022-02-20 19:59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>冬奥 开幕式 骂 偷国 选手 想 美 疫情 思考 热带 国家    震撼  中国 魂 狠...</td>\n",
       "      <td>2022-02-04 23:31</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>今年冬天  恨不能  国内  想  环球 影城 想  yyqx  电影 更想  冬奥   疫...</td>\n",
       "      <td>2022-02-04 22:55</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>冬奥 黑 那下 一届 米兰 极有   牌   不到</td>\n",
       "      <td>2022-02-20 20:37</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>爱 运动 中国  安踏 冬奥 加油 冰雪 运动 健儿  加油</td>\n",
       "      <td>2022-02-04 11:59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>今日 立春 冬奥 开幕 美国队 进场 走   太 散漫</td>\n",
       "      <td>2022-02-04 20:59</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>冰雪  温度  完 感觉  励志   冰雪 运动  魅力  真的    敬佩   年龄   ...</td>\n",
       "      <td>2022-02-04 15:59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>参加 北京 冬奥 闭幕式 运动员 数 破纪录    b 疫情</td>\n",
       "      <td>2022-02-20 20:04</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>拼过  闪耀  支持 冬奥 健儿 闪耀 出征</td>\n",
       "      <td>2022-02-10 12:59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2334 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context              time  \\\n",
       "2                           肖战 期待 冬奥 赛场    抹 中国 红 加油 加油  2022-02-04 08:58   \n",
       "4                                   北京 冬奥会 闭幕式 期待 下次 冬奥  2022-02-20 19:59   \n",
       "5       冬奥 开幕式 骂 偷国 选手 想 美 疫情 思考 热带 国家    震撼  中国 魂 狠...  2022-02-04 23:31   \n",
       "6     今年冬天  恨不能  国内  想  环球 影城 想  yyqx  电影 更想  冬奥   疫...  2022-02-04 22:55   \n",
       "7                             冬奥 黑 那下 一届 米兰 极有   牌   不到  2022-02-20 20:37   \n",
       "...                                                 ...               ...   \n",
       "3402                     爱 运动 中国  安踏 冬奥 加油 冰雪 运动 健儿  加油  2022-02-04 11:59   \n",
       "3404                      今日 立春 冬奥 开幕 美国队 进场 走   太 散漫    2022-02-04 20:59   \n",
       "3406  冰雪  温度  完 感觉  励志   冰雪 运动  魅力  真的    敬佩   年龄   ...  2022-02-04 15:59   \n",
       "3407                参加 北京 冬奥 闭幕式 运动员 数 破纪录    b 疫情       2022-02-20 20:04   \n",
       "3408                             拼过  闪耀  支持 冬奥 健儿 闪耀 出征  2022-02-10 12:59   \n",
       "\n",
       "      emotion  \n",
       "2         1.0  \n",
       "4         1.0  \n",
       "5        -1.0  \n",
       "6        -1.0  \n",
       "7        -1.0  \n",
       "...       ...  \n",
       "3402      1.0  \n",
       "3404     -1.0  \n",
       "3406      1.0  \n",
       "3407     -1.0  \n",
       "3408      1.0  \n",
       "\n",
       "[2334 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.to_csv('data_cleaned.csv',encoding='utf_8_sig')\n",
    "df_train = df_train.drop(df_train[df_train['emotion'] == 0].index)\n",
    "df_train = df_train.dropna(axis=0, how = 'all')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d32cb29-3704-4691-a3a6-6102871a5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('cleaned2.csv',encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45b0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b5f1f51-b9ec-4c47-8655-21d3fe8f42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train[['context','emotion']][0:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac79a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f041ecdb-fc91-40cf-944c-c04838695dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_train[['context','emotion']][1501:1719]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32dfedc0-ab19-4797-9e2c-db7a41519e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuwan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['元', '吨', '数', '末'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern='\\[?\\w+\\]?', \n",
    "                             stop_words=stopwords)\n",
    "X_train = vectorizer.fit_transform(train_data[\"context\"])\n",
    "y_train = train_data[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49db8e15-c159-4ef6-a7c0-448048ee5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_data[\"context\"])\n",
    "y_test = test_data[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec6272bc-fe94-4607-9e27-f721678beb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afde0470-d872-4a13-82f8-914eb7855efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "       -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5442999-664c-4281-b8a7-10c06eafb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, label) -> float:\n",
    "    # Use the code below to obtain the accuracy of your algorithm\n",
    "    error = float((output != label).sum()) * 1. / len(output)\n",
    "    print('Error: {:2.4f}%'.format(100 * error))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32cde481-23c2-4bc0-97a0-94580b074240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 18.8073%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18807339449541285"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73659836-1f10-4060-8dab-280c2443d54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8368/580928889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"准确率:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \"\"\"\n\u001b[0;32m   1969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1970\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m     83\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'f'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'continuous'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090acd2-3466-49d7-a54f-feb6f33a0795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
